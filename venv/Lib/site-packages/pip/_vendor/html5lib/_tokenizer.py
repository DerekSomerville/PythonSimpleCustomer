from __future__ import absolute_import, division, unicode_literals

from pip._vendor.six import unichr as chr

from collections import deque, OrderedDict
from sys import version_info

from .constants import space_characters
from .constants import entities
from .constants import ascii_letters, ascii_upper2_lower
from .constants import digits, hex_digits, EOF
from .constants import token_types, tag_token_types
from .constants import replacement_characters

from ._inputstream import HTMLInputStream

from ._trie import Trie

entities_trie = Trie(entities)

if version_info >= (3, 7):
    attribute_map = dict
else:
    attribute_map = OrderedDict


class HTMLTokenizer(object):
    """ This class takes care of tokenizing HTML.

    * self.current_token
      Holds the token that is currently being processed.

    * self.state
      Holds a reference to the method to be invoked... XXX

    * self.stream
      Points to HTMLInputStream object.
    """

    def __init__(self, stream, parser=None, **kwargs):

        self.stream = HTMLInputStream(stream, **kwargs)
        self.parser = parser

        # Setup the initial tokenizer state
        self.escape_flag = False
        self.last_four_chars = []
        self.state = self.data_state
        self.escape = False

        # The current token being created
        self.current_token = None
        super(HTMLTokenizer, self).__init__()

    def __iter__(self):
        """ This is where the magic happens.

        We do our usually processing through the states and when we have a token
        to return we yield the token which pauses processing until the next token
        is requested.
        """
        self.token_queue = deque([])
        # Start processing. When EOF is reached self.state will return False
        # instead of True and the loop will terminate.
        while self.state():
            while self.stream.errors:
                yield {"type": token_types["ParseError"], "data": self.stream.errors.pop(0)}
            while self.token_queue:
                yield self.token_queue.popleft()

    def consume_number_entity(self, is_hex):
        """This function returns either U+FFFD or the character based on the
        decimal or hexadecimal representation. It also discards ";" if present.
        If not present self.token_queue.append({"type": token_types["ParseError"]}) is invoked.
        """

        allowed = digits
        radix = 10
        if is_hex:
            allowed = hex_digits
            radix = 16

        char_stack = []

        # Consume all the characters that are in range while making sure we
        # don't hit an EOF.
        c = self.stream.char()
        while c in allowed and c is not EOF:
            char_stack.append(c)
            c = self.stream.char()

        # Convert the set of characters consumed to an int.
        char_as_int = int("".join(char_stack), radix)

        # Certain characters get replaced with others
        if char_as_int in replacement_characters:
            char = replacement_characters[char_as_int]
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "illegal-codepoint-for-numeric-entity",
                                    "datavars": {"char_as_int": char_as_int}})
        elif ((0xD800 <= char_as_int <= 0xDFFF) or
              (char_as_int > 0x10FFFF)):
            char = "\u_fffd"
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "illegal-codepoint-for-numeric-entity",
                                    "datavars": {"char_as_int": char_as_int}})
        else:
            # Should speed up this check somehow (e.g. move the set to a constant)
            if ((0x0001 <= char_as_int <= 0x0008) or
                (0x000E <= char_as_int <= 0x001F) or
                (0x007F <= char_as_int <= 0x009F) or
                (0xFDD0 <= char_as_int <= 0xFDEF) or
                char_as_int in frozenset([0x000B, 0xFFFE, 0xFFFF, 0x1FFFE,
                                        0x1FFFF, 0x2FFFE, 0x2FFFF, 0x3FFFE,
                                        0x3FFFF, 0x4FFFE, 0x4FFFF, 0x5FFFE,
                                        0x5FFFF, 0x6FFFE, 0x6FFFF, 0x7FFFE,
                                        0x7FFFF, 0x8FFFE, 0x8FFFF, 0x9FFFE,
                                        0x9FFFF, 0xAFFFE, 0xAFFFF, 0xBFFFE,
                                        0xBFFFF, 0xCFFFE, 0xCFFFF, 0xDFFFE,
                                        0xDFFFF, 0xEFFFE, 0xEFFFF, 0xFFFFE,
                                        0xFFFFF, 0x10FFFE, 0x10FFFF])):
                self.token_queue.append({"type": token_types["ParseError"],
                                        "data":
                                        "illegal-codepoint-for-numeric-entity",
                                        "datavars": {"char_as_int": char_as_int}})
            try:
                # Try/except needed as UCS-2 Python builds' unichar only works
                # within the BMP.
                char = chr(char_as_int)
            except ValueError:
                v = char_as_int - 0x10000
                char = chr(0xD800 | (v >> 10)) + chr(0xDC00 | (v & 0x3FF))

        # Discard the ; if present. Otherwise, put it back on the queue and
        # invoke parse_error on parser.
        if c != ";":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "numeric-entity-without-semicolon"})
            self.stream.unget(c)

        return char

    def consume_entity(self, allowed_char=None, from_attribute=False):
        # Initialise to the default output for when no entity is matched
        output = "&"

        char_stack = [self.stream.char()]
        if (char_stack[0] in space_characters or char_stack[0] in (EOF, "<", "&") or
                (allowed_char is not None and allowed_char == char_stack[0])):
            self.stream.unget(char_stack[0])

        elif char_stack[0] == "#":
            # Read the next character to see if it's hex or decimal
            hex = False
            char_stack.append(self.stream.char())
            if char_stack[-1] in ("x", "X"):
                hex = True
                char_stack.append(self.stream.char())

            # char_stack[-1] should be the first digit
            if (hex and char_stack[-1] in hex_digits) \
                    or (not hex and char_stack[-1] in digits):
                # At least one digit found, so consume the whole number
                self.stream.unget(char_stack[-1])
                output = self.consume_number_entity(hex)
            else:
                # No digits found
                self.token_queue.append({"type": token_types["ParseError"],
                                        "data": "expected-numeric-entity"})
                self.stream.unget(char_stack.pop())
                output = "&" + "".join(char_stack)

        else:
            # At this point in the process might have named entity. Entities
            # are stored in the global variable "entities".
            #
            # Consume characters and compare to these to a substring of the
            # entity names in the list until the substring no longer matches.
            while (char_stack[-1] is not EOF):
                if not entities_trie.has_keys_with_prefix("".join(char_stack)):
                    break
                char_stack.append(self.stream.char())

            # At this point we have a string that starts with some characters
            # that may match an entity
            # Try to find the longest entity the string will match to take care
            # of &noti for instance.
            try:
                entity_name = entities_trie.longest_prefix("".join(char_stack[:-1]))
                entity_length = len(entity_name)
            except KeyError:
                entity_name = None

            if entity_name is not None:
                if entity_name[-1] != ";":
                    self.token_queue.append({"type": token_types["ParseError"], "data":
                                            "named-entity-without-semicolon"})
                if (entity_name[-1] != ";" and from_attribute and
                    (char_stack[entity_length] in ascii_letters or
                     char_stack[entity_length] in digits or
                     char_stack[entity_length] == "=")):
                    self.stream.unget(char_stack.pop())
                    output = "&" + "".join(char_stack)
                else:
                    output = entities[entity_name]
                    self.stream.unget(char_stack.pop())
                    output += "".join(char_stack[entity_length:])
            else:
                self.token_queue.append({"type": token_types["ParseError"], "data":
                                        "expected-named-entity"})
                self.stream.unget(char_stack.pop())
                output = "&" + "".join(char_stack)

        if from_attribute:
            self.current_token["data"][-1][1] += output
        else:
            if output in space_characters:
                token_type = "SpaceCharacters"
            else:
                token_type = "Characters"
            self.token_queue.append({"type": token_types[token_type], "data": output})

    def process_entity_in_attribute(self, allowed_char):
        """This method replaces the need for "entity_in_attribute_value_state".
        """
        self.consume_entity(allowed_char=allowed_char, from_attribute=True)

    def emit_current_token(self):
        """This method is a generic handler for emitting the tags. It also sets
        the state to "data" because that's what's needed after a token has been
        emitted.
        """
        token = self.current_token
        # Add token to the queue to be yielded
        if (token["type"] in tag_token_types):
            token["name"] = token["name"].translate(ascii_upper2_lower)
            if token["type"] == token_types["StartTag"]:
                raw = token["data"]
                data = attribute_map(raw)
                if len(raw) > len(data):
                    # we had some duplicated attribute, fix so first wins
                    data.update(raw[::-1])
                token["data"] = data

            if token["type"] == token_types["EndTag"]:
                if token["data"]:
                    self.token_queue.append({"type": token_types["ParseError"],
                                            "data": "attributes-in-end-tag"})
                if token["self_closing"]:
                    self.token_queue.append({"type": token_types["ParseError"],
                                            "data": "self-closing-flag-on-end-tag"})
        self.token_queue.append(token)
        self.state = self.data_state

    # Below are the various tokenizer states worked out.
    def data_state(self):
        data = self.stream.char()
        if data == "&":
            self.state = self.entity_data_state
        elif data == "<":
            self.state = self.tag_open_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u0000"})
        elif data is EOF:
            # Tokenization ends.
            return False
        elif data in space_characters:
            # Directly after emitting a token you switch back to the "data
            # state". At that point space_characters are important so they are
            # emitted separately.
            self.token_queue.append({"type": token_types["SpaceCharacters"], "data":
                                    data + self.stream.chars_until(space_characters, True)})
            # No need to update last_four_chars here, since the first space will
            # have already been appended to last_four_chars and will have broken
            # any <!-- or --> sequences
        else:
            chars = self.stream.chars_until(("&", "<", "\u0000"))
            self.token_queue.append({"type": token_types["Characters"], "data":
                                    data + chars})
        return True

    def entity_data_state(self):
        self.consume_entity()
        self.state = self.data_state
        return True

    def rcdata_state(self):
        data = self.stream.char()
        if data == "&":
            self.state = self.character_reference_in_rcdata
        elif data == "<":
            self.state = self.rcdata_less_than_sign_state
        elif data == EOF:
            # Tokenization ends.
            return False
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
        elif data in space_characters:
            # Directly after emitting a token you switch back to the "data
            # state". At that point space_characters are important so they are
            # emitted separately.
            self.token_queue.append({"type": token_types["SpaceCharacters"], "data":
                                    data + self.stream.chars_until(space_characters, True)})
            # No need to update last_four_chars here, since the first space will
            # have already been appended to last_four_chars and will have broken
            # any <!-- or --> sequences
        else:
            chars = self.stream.chars_until(("&", "<", "\u0000"))
            self.token_queue.append({"type": token_types["Characters"], "data":
                                    data + chars})
        return True

    def character_reference_in_rcdata(self):
        self.consume_entity()
        self.state = self.rcdata_state
        return True

    def rawtext_state(self):
        data = self.stream.char()
        if data == "<":
            self.state = self.rawtext_less_than_sign_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
        elif data == EOF:
            # Tokenization ends.
            return False
        else:
            chars = self.stream.chars_until(("<", "\u0000"))
            self.token_queue.append({"type": token_types["Characters"], "data":
                                    data + chars})
        return True

    def script_data_state(self):
        data = self.stream.char()
        if data == "<":
            self.state = self.script_data_less_than_sign_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
        elif data == EOF:
            # Tokenization ends.
            return False
        else:
            chars = self.stream.chars_until(("<", "\u0000"))
            self.token_queue.append({"type": token_types["Characters"], "data":
                                    data + chars})
        return True

    def plaintext_state(self):
        data = self.stream.char()
        if data == EOF:
            # Tokenization ends.
            return False
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
        else:
            self.token_queue.append({"type": token_types["Characters"], "data":
                                    data + self.stream.chars_until("\u0000")})
        return True

    def tag_open_state(self):
        data = self.stream.char()
        if data == "!":
            self.state = self.markup_declaration_open_state
        elif data == "/":
            self.state = self.close_tag_open_state
        elif data in ascii_letters:
            self.current_token = {"type": token_types["StartTag"],
                                 "name": data, "data": [],
                                 "self_closing": False,
                                 "self_closing_acknowledged": False}
            self.state = self.tag_name_state
        elif data == ">":
            # XXX In theory it could be something besides a tag name. But
            # do we really care?
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-tag-name-but-got-right-bracket"})
            self.token_queue.append({"type": token_types["Characters"], "data": "<>"})
            self.state = self.data_state
        elif data == "?":
            # XXX In theory it could be something besides a tag name. But
            # do we really care?
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-tag-name-but-got-question-mark"})
            self.stream.unget(data)
            self.state = self.bogus_comment_state
        else:
            # XXX
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-tag-name"})
            self.token_queue.append({"type": token_types["Characters"], "data": "<"})
            self.stream.unget(data)
            self.state = self.data_state
        return True

    def close_tag_open_state(self):
        data = self.stream.char()
        if data in ascii_letters:
            self.current_token = {"type": token_types["EndTag"], "name": data,
                                 "data": [], "self_closing": False}
            self.state = self.tag_name_state
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-closing-tag-but-got-right-bracket"})
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-closing-tag-but-got-eof"})
            self.token_queue.append({"type": token_types["Characters"], "data": "</"})
            self.state = self.data_state
        else:
            # XXX data can be _'_...
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-closing-tag-but-got-char",
                                    "datavars": {"data": data}})
            self.stream.unget(data)
            self.state = self.bogus_comment_state
        return True

    def tag_name_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.state = self.before_attribute_name_state
        elif data == ">":
            self.emit_current_token()
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-tag-name"})
            self.state = self.data_state
        elif data == "/":
            self.state = self.self_closing_start_tag_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["name"] += "\u_fffd"
        else:
            self.current_token["name"] += data
            # (Don't use chars_until here, because tag names are
            # very short and it's faster to not do anything fancy)
        return True

    def rcdata_less_than_sign_state(self):
        data = self.stream.char()
        if data == "/":
            self.temporary_buffer = ""
            self.state = self.rcdata_end_tag_open_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": "<"})
            self.stream.unget(data)
            self.state = self.rcdata_state
        return True

    def rcdata_end_tag_open_state(self):
        data = self.stream.char()
        if data in ascii_letters:
            self.temporary_buffer += data
            self.state = self.rcdata_end_tag_name_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": "</"})
            self.stream.unget(data)
            self.state = self.rcdata_state
        return True

    def rcdata_end_tag_name_state(self):
        appropriate = self.current_token and self.current_token["name"].lower() == self.temporary_buffer.lower()
        data = self.stream.char()
        if data in space_characters and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.state = self.before_attribute_name_state
        elif data == "/" and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.state = self.self_closing_start_tag_state
        elif data == ">" and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.emit_current_token()
            self.state = self.data_state
        elif data in ascii_letters:
            self.temporary_buffer += data
        else:
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "</" + self.temporary_buffer})
            self.stream.unget(data)
            self.state = self.rcdata_state
        return True

    def rawtext_less_than_sign_state(self):
        data = self.stream.char()
        if data == "/":
            self.temporary_buffer = ""
            self.state = self.rawtext_end_tag_open_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": "<"})
            self.stream.unget(data)
            self.state = self.rawtext_state
        return True

    def rawtext_end_tag_open_state(self):
        data = self.stream.char()
        if data in ascii_letters:
            self.temporary_buffer += data
            self.state = self.rawtext_end_tag_name_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": "</"})
            self.stream.unget(data)
            self.state = self.rawtext_state
        return True

    def rawtext_end_tag_name_state(self):
        appropriate = self.current_token and self.current_token["name"].lower() == self.temporary_buffer.lower()
        data = self.stream.char()
        if data in space_characters and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.state = self.before_attribute_name_state
        elif data == "/" and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.state = self.self_closing_start_tag_state
        elif data == ">" and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.emit_current_token()
            self.state = self.data_state
        elif data in ascii_letters:
            self.temporary_buffer += data
        else:
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "</" + self.temporary_buffer})
            self.stream.unget(data)
            self.state = self.rawtext_state
        return True

    def script_data_less_than_sign_state(self):
        data = self.stream.char()
        if data == "/":
            self.temporary_buffer = ""
            self.state = self.script_data_end_tag_open_state
        elif data == "!":
            self.token_queue.append({"type": token_types["Characters"], "data": "<!"})
            self.state = self.script_data_escape_start_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": "<"})
            self.stream.unget(data)
            self.state = self.script_data_state
        return True

    def script_data_end_tag_open_state(self):
        data = self.stream.char()
        if data in ascii_letters:
            self.temporary_buffer += data
            self.state = self.script_data_end_tag_name_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": "</"})
            self.stream.unget(data)
            self.state = self.script_data_state
        return True

    def script_data_end_tag_name_state(self):
        appropriate = self.current_token and self.current_token["name"].lower() == self.temporary_buffer.lower()
        data = self.stream.char()
        if data in space_characters and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.state = self.before_attribute_name_state
        elif data == "/" and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.state = self.self_closing_start_tag_state
        elif data == ">" and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.emit_current_token()
            self.state = self.data_state
        elif data in ascii_letters:
            self.temporary_buffer += data
        else:
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "</" + self.temporary_buffer})
            self.stream.unget(data)
            self.state = self.script_data_state
        return True

    def script_data_escape_start_state(self):
        data = self.stream.char()
        if data == "-":
            self.token_queue.append({"type": token_types["Characters"], "data": "-"})
            self.state = self.script_data_escape_start_dash_state
        else:
            self.stream.unget(data)
            self.state = self.script_data_state
        return True

    def script_data_escape_start_dash_state(self):
        data = self.stream.char()
        if data == "-":
            self.token_queue.append({"type": token_types["Characters"], "data": "-"})
            self.state = self.script_data_escaped_dash_dash_state
        else:
            self.stream.unget(data)
            self.state = self.script_data_state
        return True

    def script_data_escaped_state(self):
        data = self.stream.char()
        if data == "-":
            self.token_queue.append({"type": token_types["Characters"], "data": "-"})
            self.state = self.script_data_escaped_dash_state
        elif data == "<":
            self.state = self.script_data_escaped_less_than_sign_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
        elif data == EOF:
            self.state = self.data_state
        else:
            chars = self.stream.chars_until(("<", "-", "\u0000"))
            self.token_queue.append({"type": token_types["Characters"], "data":
                                    data + chars})
        return True

    def script_data_escaped_dash_state(self):
        data = self.stream.char()
        if data == "-":
            self.token_queue.append({"type": token_types["Characters"], "data": "-"})
            self.state = self.script_data_escaped_dash_dash_state
        elif data == "<":
            self.state = self.script_data_escaped_less_than_sign_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
            self.state = self.script_data_escaped_state
        elif data == EOF:
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": data})
            self.state = self.script_data_escaped_state
        return True

    def script_data_escaped_dash_dash_state(self):
        data = self.stream.char()
        if data == "-":
            self.token_queue.append({"type": token_types["Characters"], "data": "-"})
        elif data == "<":
            self.state = self.script_data_escaped_less_than_sign_state
        elif data == ">":
            self.token_queue.append({"type": token_types["Characters"], "data": ">"})
            self.state = self.script_data_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
            self.state = self.script_data_escaped_state
        elif data == EOF:
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": data})
            self.state = self.script_data_escaped_state
        return True

    def script_data_escaped_less_than_sign_state(self):
        data = self.stream.char()
        if data == "/":
            self.temporary_buffer = ""
            self.state = self.script_data_escaped_end_tag_open_state
        elif data in ascii_letters:
            self.token_queue.append({"type": token_types["Characters"], "data": "<" + data})
            self.temporary_buffer = data
            self.state = self.script_data_double_escape_start_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": "<"})
            self.stream.unget(data)
            self.state = self.script_data_escaped_state
        return True

    def script_data_escaped_end_tag_open_state(self):
        data = self.stream.char()
        if data in ascii_letters:
            self.temporary_buffer = data
            self.state = self.script_data_escaped_end_tag_name_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": "</"})
            self.stream.unget(data)
            self.state = self.script_data_escaped_state
        return True

    def script_data_escaped_end_tag_name_state(self):
        appropriate = self.current_token and self.current_token["name"].lower() == self.temporary_buffer.lower()
        data = self.stream.char()
        if data in space_characters and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.state = self.before_attribute_name_state
        elif data == "/" and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.state = self.self_closing_start_tag_state
        elif data == ">" and appropriate:
            self.current_token = {"type": token_types["EndTag"],
                                 "name": self.temporary_buffer,
                                 "data": [], "self_closing": False}
            self.emit_current_token()
            self.state = self.data_state
        elif data in ascii_letters:
            self.temporary_buffer += data
        else:
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "</" + self.temporary_buffer})
            self.stream.unget(data)
            self.state = self.script_data_escaped_state
        return True

    def script_data_double_escape_start_state(self):
        data = self.stream.char()
        if data in (space_characters | frozenset(("/", ">"))):
            self.token_queue.append({"type": token_types["Characters"], "data": data})
            if self.temporary_buffer.lower() == "script":
                self.state = self.script_data_double_escaped_state
            else:
                self.state = self.script_data_escaped_state
        elif data in ascii_letters:
            self.token_queue.append({"type": token_types["Characters"], "data": data})
            self.temporary_buffer += data
        else:
            self.stream.unget(data)
            self.state = self.script_data_escaped_state
        return True

    def script_data_double_escaped_state(self):
        data = self.stream.char()
        if data == "-":
            self.token_queue.append({"type": token_types["Characters"], "data": "-"})
            self.state = self.script_data_double_escaped_dash_state
        elif data == "<":
            self.token_queue.append({"type": token_types["Characters"], "data": "<"})
            self.state = self.script_data_double_escaped_less_than_sign_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
        elif data == EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-script-in-script"})
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": data})
        return True

    def script_data_double_escaped_dash_state(self):
        data = self.stream.char()
        if data == "-":
            self.token_queue.append({"type": token_types["Characters"], "data": "-"})
            self.state = self.script_data_double_escaped_dash_dash_state
        elif data == "<":
            self.token_queue.append({"type": token_types["Characters"], "data": "<"})
            self.state = self.script_data_double_escaped_less_than_sign_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
            self.state = self.script_data_double_escaped_state
        elif data == EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-script-in-script"})
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": data})
            self.state = self.script_data_double_escaped_state
        return True

    def script_data_double_escaped_dash_dash_state(self):
        data = self.stream.char()
        if data == "-":
            self.token_queue.append({"type": token_types["Characters"], "data": "-"})
        elif data == "<":
            self.token_queue.append({"type": token_types["Characters"], "data": "<"})
            self.state = self.script_data_double_escaped_less_than_sign_state
        elif data == ">":
            self.token_queue.append({"type": token_types["Characters"], "data": ">"})
            self.state = self.script_data_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": "\u_fffd"})
            self.state = self.script_data_double_escaped_state
        elif data == EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-script-in-script"})
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["Characters"], "data": data})
            self.state = self.script_data_double_escaped_state
        return True

    def script_data_double_escaped_less_than_sign_state(self):
        data = self.stream.char()
        if data == "/":
            self.token_queue.append({"type": token_types["Characters"], "data": "/"})
            self.temporary_buffer = ""
            self.state = self.script_data_double_escape_end_state
        else:
            self.stream.unget(data)
            self.state = self.script_data_double_escaped_state
        return True

    def script_data_double_escape_end_state(self):
        data = self.stream.char()
        if data in (space_characters | frozenset(("/", ">"))):
            self.token_queue.append({"type": token_types["Characters"], "data": data})
            if self.temporary_buffer.lower() == "script":
                self.state = self.script_data_escaped_state
            else:
                self.state = self.script_data_double_escaped_state
        elif data in ascii_letters:
            self.token_queue.append({"type": token_types["Characters"], "data": data})
            self.temporary_buffer += data
        else:
            self.stream.unget(data)
            self.state = self.script_data_double_escaped_state
        return True

    def before_attribute_name_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.stream.chars_until(space_characters, True)
        elif data in ascii_letters:
            self.current_token["data"].append([data, ""])
            self.state = self.attribute_name_state
        elif data == ">":
            self.emit_current_token()
        elif data == "/":
            self.state = self.self_closing_start_tag_state
        elif data in ("'", '"', "=", "<"):
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "invalid-character-in-attribute-name"})
            self.current_token["data"].append([data, ""])
            self.state = self.attribute_name_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"].append(["\u_fffd", ""])
            self.state = self.attribute_name_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-attribute-name-but-got-eof"})
            self.state = self.data_state
        else:
            self.current_token["data"].append([data, ""])
            self.state = self.attribute_name_state
        return True

    def attribute_name_state(self):
        data = self.stream.char()
        leaving_this_state = True
        emit_token = False
        if data == "=":
            self.state = self.before_attribute_value_state
        elif data in ascii_letters:
            self.current_token["data"][-1][0] += data +\
                self.stream.chars_until(ascii_letters, True)
            leaving_this_state = False
        elif data == ">":
            # XXX If we emit here the attributes are converted to a dict
            # without being checked and when the code below runs we error
            # because data is a dict not a list
            emit_token = True
        elif data in space_characters:
            self.state = self.after_attribute_name_state
        elif data == "/":
            self.state = self.self_closing_start_tag_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"][-1][0] += "\u_fffd"
            leaving_this_state = False
        elif data in ("'", '"', "<"):
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data":
                                    "invalid-character-in-attribute-name"})
            self.current_token["data"][-1][0] += data
            leaving_this_state = False
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "eof-in-attribute-name"})
            self.state = self.data_state
        else:
            self.current_token["data"][-1][0] += data
            leaving_this_state = False

        if leaving_this_state:
            # Attributes are not dropped at this stage. That happens when the
            # start tag token is emitted so values can still be safely appended
            # to attributes, but we do want to report the parse error in time.
            self.current_token["data"][-1][0] = (
                self.current_token["data"][-1][0].translate(ascii_upper2_lower))
            for name, _ in self.current_token["data"][:-1]:
                if self.current_token["data"][-1][0] == name:
                    self.token_queue.append({"type": token_types["ParseError"], "data":
                                            "duplicate-attribute"})
                    break
            # XXX Fix for above XXX
            if emit_token:
                self.emit_current_token()
        return True

    def after_attribute_name_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.stream.chars_until(space_characters, True)
        elif data == "=":
            self.state = self.before_attribute_value_state
        elif data == ">":
            self.emit_current_token()
        elif data in ascii_letters:
            self.current_token["data"].append([data, ""])
            self.state = self.attribute_name_state
        elif data == "/":
            self.state = self.self_closing_start_tag_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"].append(["\u_fffd", ""])
            self.state = self.attribute_name_state
        elif data in ("'", '"', "<"):
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "invalid-character-after-attribute-name"})
            self.current_token["data"].append([data, ""])
            self.state = self.attribute_name_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-end-of-tag-but-got-eof"})
            self.state = self.data_state
        else:
            self.current_token["data"].append([data, ""])
            self.state = self.attribute_name_state
        return True

    def before_attribute_value_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.stream.chars_until(space_characters, True)
        elif data == "\"":
            self.state = self.attribute_value_double_quoted_state
        elif data == "&":
            self.state = self.attribute_value_un_quoted_state
            self.stream.unget(data)
        elif data == "'":
            self.state = self.attribute_value_single_quoted_state
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-attribute-value-but-got-right-bracket"})
            self.emit_current_token()
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"][-1][1] += "\u_fffd"
            self.state = self.attribute_value_un_quoted_state
        elif data in ("=", "<", "`"):
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "equals-in-unquoted-attribute-value"})
            self.current_token["data"][-1][1] += data
            self.state = self.attribute_value_un_quoted_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-attribute-value-but-got-eof"})
            self.state = self.data_state
        else:
            self.current_token["data"][-1][1] += data
            self.state = self.attribute_value_un_quoted_state
        return True

    def attribute_value_double_quoted_state(self):
        data = self.stream.char()
        if data == "\"":
            self.state = self.after_attribute_value_state
        elif data == "&":
            self.process_entity_in_attribute('"')
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"][-1][1] += "\u_fffd"
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-attribute-value-double-quote"})
            self.state = self.data_state
        else:
            self.current_token["data"][-1][1] += data +\
                self.stream.chars_until(("\"", "&", "\u0000"))
        return True

    def attribute_value_single_quoted_state(self):
        data = self.stream.char()
        if data == "'":
            self.state = self.after_attribute_value_state
        elif data == "&":
            self.process_entity_in_attribute("'")
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"][-1][1] += "\u_fffd"
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-attribute-value-single-quote"})
            self.state = self.data_state
        else:
            self.current_token["data"][-1][1] += data +\
                self.stream.chars_until(("'", "&", "\u0000"))
        return True

    def attribute_value_un_quoted_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.state = self.before_attribute_name_state
        elif data == "&":
            self.process_entity_in_attribute(">")
        elif data == ">":
            self.emit_current_token()
        elif data in ('"', "'", "=", "<", "`"):
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-character-in-unquoted-attribute-value"})
            self.current_token["data"][-1][1] += data
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"][-1][1] += "\u_fffd"
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-attribute-value-no-quotes"})
            self.state = self.data_state
        else:
            self.current_token["data"][-1][1] += data + self.stream.chars_until(
                frozenset(("&", ">", '"', "'", "=", "<", "`", "\u0000")) | space_characters)
        return True

    def after_attribute_value_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.state = self.before_attribute_name_state
        elif data == ">":
            self.emit_current_token()
        elif data == "/":
            self.state = self.self_closing_start_tag_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-EOF-after-attribute-value"})
            self.stream.unget(data)
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-character-after-attribute-value"})
            self.stream.unget(data)
            self.state = self.before_attribute_name_state
        return True

    def self_closing_start_tag_state(self):
        data = self.stream.char()
        if data == ">":
            self.current_token["self_closing"] = True
            self.emit_current_token()
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data":
                                    "unexpected-EOF-after-solidus-in-tag"})
            self.stream.unget(data)
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-character-after-solidus-in-tag"})
            self.stream.unget(data)
            self.state = self.before_attribute_name_state
        return True

    def bogus_comment_state(self):
        # Make a new comment token and give it as value all the characters
        # until the first > or EOF (chars_until checks for EOF automatically)
        # and emit it.
        data = self.stream.chars_until(">")
        data = data.replace("\u0000", "\u_fffd")
        self.token_queue.append(
            {"type": token_types["Comment"], "data": data})

        # Eat the character directly after the bogus comment which is either a
        # ">" or an EOF.
        self.stream.char()
        self.state = self.data_state
        return True

    def markup_declaration_open_state(self):
        char_stack = [self.stream.char()]
        if char_stack[-1] == "-":
            char_stack.append(self.stream.char())
            if char_stack[-1] == "-":
                self.current_token = {"type": token_types["Comment"], "data": ""}
                self.state = self.comment_start_state
                return True
        elif char_stack[-1] in ('d', 'D'):
            matched = True
            for expected in (('o', 'O'), ('c', 'C'), ('t', 'T'),
                             ('y', 'Y'), ('p', 'P'), ('e', 'E')):
                char_stack.append(self.stream.char())
                if char_stack[-1] not in expected:
                    matched = False
                    break
            if matched:
                self.current_token = {"type": token_types["Doctype"],
                                     "name": "",
                                     "public_id": None, "system_id": None,
                                     "correct": True}
                self.state = self.doctype_state
                return True
        elif (char_stack[-1] == "[" and
              self.parser is not None and
              self.parser.tree.open_elements and
              self.parser.tree.open_elements[-1].namespace != self.parser.tree.default_namespace):
            matched = True
            for expected in ["C", "D", "A", "T", "A", "["]:
                char_stack.append(self.stream.char())
                if char_stack[-1] != expected:
                    matched = False
                    break
            if matched:
                self.state = self.cdata_section_state
                return True

        self.token_queue.append({"type": token_types["ParseError"], "data":
                                "expected-dashes-or-doctype"})

        while char_stack:
            self.stream.unget(char_stack.pop())
        self.state = self.bogus_comment_state
        return True

    def comment_start_state(self):
        data = self.stream.char()
        if data == "-":
            self.state = self.comment_start_dash_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"] += "\u_fffd"
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "incorrect-comment"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-comment"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["data"] += data
            self.state = self.comment_state
        return True

    def comment_start_dash_state(self):
        data = self.stream.char()
        if data == "-":
            self.state = self.comment_end_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"] += "-\u_fffd"
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "incorrect-comment"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-comment"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["data"] += "-" + data
            self.state = self.comment_state
        return True

    def comment_state(self):
        data = self.stream.char()
        if data == "-":
            self.state = self.comment_end_dash_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"] += "\u_fffd"
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "eof-in-comment"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["data"] += data + \
                self.stream.chars_until(("-", "\u0000"))
        return True

    def comment_end_dash_state(self):
        data = self.stream.char()
        if data == "-":
            self.state = self.comment_end_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"] += "-\u_fffd"
            self.state = self.comment_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-comment-end-dash"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["data"] += "-" + data
            self.state = self.comment_state
        return True

    def comment_end_state(self):
        data = self.stream.char()
        if data == ">":
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"] += "--\u_fffd"
            self.state = self.comment_state
        elif data == "!":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-bang-after-double-dash-in-comment"})
            self.state = self.comment_end_bang_state
        elif data == "-":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-dash-after-double-dash-in-comment"})
            self.current_token["data"] += data
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-comment-double-dash"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            # XXX
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-comment"})
            self.current_token["data"] += "--" + data
            self.state = self.comment_state
        return True

    def comment_end_bang_state(self):
        data = self.stream.char()
        if data == ">":
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data == "-":
            self.current_token["data"] += "--!"
            self.state = self.comment_end_dash_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["data"] += "--!\u_fffd"
            self.state = self.comment_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-comment-end-bang-state"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["data"] += "--!" + data
            self.state = self.comment_state
        return True

    def doctype_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.state = self.before_doctype_name_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-doctype-name-but-got-eof"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "need-space-after-doctype"})
            self.stream.unget(data)
            self.state = self.before_doctype_name_state
        return True

    def before_doctype_name_state(self):
        data = self.stream.char()
        if data in space_characters:
            pass
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-doctype-name-but-got-right-bracket"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["name"] = "\u_fffd"
            self.state = self.doctype_name_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-doctype-name-but-got-eof"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["name"] = data
            self.state = self.doctype_name_state
        return True

    def doctype_name_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.current_token["name"] = self.current_token["name"].translate(ascii_upper2_lower)
            self.state = self.after_doctype_name_state
        elif data == ">":
            self.current_token["name"] = self.current_token["name"].translate(ascii_upper2_lower)
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["name"] += "\u_fffd"
            self.state = self.doctype_name_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype-name"})
            self.current_token["correct"] = False
            self.current_token["name"] = self.current_token["name"].translate(ascii_upper2_lower)
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["name"] += data
        return True

    def after_doctype_name_state(self):
        data = self.stream.char()
        if data in space_characters:
            pass
        elif data == ">":
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.current_token["correct"] = False
            self.stream.unget(data)
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            if data in ("p", "P"):
                matched = True
                for expected in (("u", "U"), ("b", "B"), ("l", "L"),
                                 ("i", "I"), ("c", "C")):
                    data = self.stream.char()
                    if data not in expected:
                        matched = False
                        break
                if matched:
                    self.state = self.after_doctype_public_keyword_state
                    return True
            elif data in ("s", "S"):
                matched = True
                for expected in (("y", "Y"), ("s", "S"), ("t", "T"),
                                 ("e", "E"), ("m", "M")):
                    data = self.stream.char()
                    if data not in expected:
                        matched = False
                        break
                if matched:
                    self.state = self.after_doctype_system_keyword_state
                    return True

            # All the characters read before the current 'data' will be
            # [a-z_a-Z], so they're garbage in the bogus doctype and can be
            # discarded; only the latest character might be '>' or EOF
            # and needs to be ungetted
            self.stream.unget(data)
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "expected-space-or-right-bracket-in-doctype", "datavars":
                                    {"data": data}})
            self.current_token["correct"] = False
            self.state = self.bogus_doctype_state

        return True

    def after_doctype_public_keyword_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.state = self.before_doctype_public_identifier_state
        elif data in ("'", '"'):
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.stream.unget(data)
            self.state = self.before_doctype_public_identifier_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.stream.unget(data)
            self.state = self.before_doctype_public_identifier_state
        return True

    def before_doctype_public_identifier_state(self):
        data = self.stream.char()
        if data in space_characters:
            pass
        elif data == "\"":
            self.current_token["public_id"] = ""
            self.state = self.doctype_public_identifier_double_quoted_state
        elif data == "'":
            self.current_token["public_id"] = ""
            self.state = self.doctype_public_identifier_single_quoted_state
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-end-of-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.current_token["correct"] = False
            self.state = self.bogus_doctype_state
        return True

    def doctype_public_identifier_double_quoted_state(self):
        data = self.stream.char()
        if data == "\"":
            self.state = self.after_doctype_public_identifier_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["public_id"] += "\u_fffd"
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-end-of-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["public_id"] += data
        return True

    def doctype_public_identifier_single_quoted_state(self):
        data = self.stream.char()
        if data == "'":
            self.state = self.after_doctype_public_identifier_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["public_id"] += "\u_fffd"
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-end-of-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["public_id"] += data
        return True

    def after_doctype_public_identifier_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.state = self.between_doctype_public_and_system_identifiers_state
        elif data == ">":
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data == '"':
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.current_token["system_id"] = ""
            self.state = self.doctype_system_identifier_double_quoted_state
        elif data == "'":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.current_token["system_id"] = ""
            self.state = self.doctype_system_identifier_single_quoted_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.current_token["correct"] = False
            self.state = self.bogus_doctype_state
        return True

    def between_doctype_public_and_system_identifiers_state(self):
        data = self.stream.char()
        if data in space_characters:
            pass
        elif data == ">":
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data == '"':
            self.current_token["system_id"] = ""
            self.state = self.doctype_system_identifier_double_quoted_state
        elif data == "'":
            self.current_token["system_id"] = ""
            self.state = self.doctype_system_identifier_single_quoted_state
        elif data == EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.current_token["correct"] = False
            self.state = self.bogus_doctype_state
        return True

    def after_doctype_system_keyword_state(self):
        data = self.stream.char()
        if data in space_characters:
            self.state = self.before_doctype_system_identifier_state
        elif data in ("'", '"'):
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.stream.unget(data)
            self.state = self.before_doctype_system_identifier_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.stream.unget(data)
            self.state = self.before_doctype_system_identifier_state
        return True

    def before_doctype_system_identifier_state(self):
        data = self.stream.char()
        if data in space_characters:
            pass
        elif data == "\"":
            self.current_token["system_id"] = ""
            self.state = self.doctype_system_identifier_double_quoted_state
        elif data == "'":
            self.current_token["system_id"] = ""
            self.state = self.doctype_system_identifier_single_quoted_state
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.current_token["correct"] = False
            self.state = self.bogus_doctype_state
        return True

    def doctype_system_identifier_double_quoted_state(self):
        data = self.stream.char()
        if data == "\"":
            self.state = self.after_doctype_system_identifier_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["system_id"] += "\u_fffd"
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-end-of-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["system_id"] += data
        return True

    def doctype_system_identifier_single_quoted_state(self):
        data = self.stream.char()
        if data == "'":
            self.state = self.after_doctype_system_identifier_state
        elif data == "\u0000":
            self.token_queue.append({"type": token_types["ParseError"],
                                    "data": "invalid-codepoint"})
            self.current_token["system_id"] += "\u_fffd"
        elif data == ">":
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-end-of-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.current_token["system_id"] += data
        return True

    def after_doctype_system_identifier_state(self):
        data = self.stream.char()
        if data in space_characters:
            pass
        elif data == ">":
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "eof-in-doctype"})
            self.current_token["correct"] = False
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            self.token_queue.append({"type": token_types["ParseError"], "data":
                                    "unexpected-char-in-doctype"})
            self.state = self.bogus_doctype_state
        return True

    def bogus_doctype_state(self):
        data = self.stream.char()
        if data == ">":
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        elif data is EOF:
            # XXX EMIT
            self.stream.unget(data)
            self.token_queue.append(self.current_token)
            self.state = self.data_state
        else:
            pass
        return True

    def cdata_section_state(self):
        data = []
        while True:
            data.append(self.stream.chars_until("]"))
            data.append(self.stream.chars_until(">"))
            char = self.stream.char()
            if char == EOF:
                break
            else:
                assert char == ">"
                if data[-1][-2:] == "]]":
                    data[-1] = data[-1][:-2]
                    break
                else:
                    data.append(char)

        data = "".join(data)  # pylint:disable=redefined-variable-type
        # Deal with null here rather than in the parser
        null_count = data.count("\u0000")
        if null_count > 0:
            for _ in range(null_count):
                self.token_queue.append({"type": token_types["ParseError"],
                                        "data": "invalid-codepoint"})
            data = data.replace("\u0000", "\u_fffd")
        if data:
            self.token_queue.append({"type": token_types["Characters"],
                                    "data": data})
        self.state = self.data_state
        return True
